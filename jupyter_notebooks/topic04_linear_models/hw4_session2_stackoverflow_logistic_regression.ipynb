{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'html', 'python', 'jquery', 'javascript', 'ios', 'c#', 'java', 'c++', 'php', 'android'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoida(z):\n",
    "    #z = z.flatten()\n",
    "    #z[z > 100] = 100\n",
    "    #z[z < -100] = -100\n",
    "    #z = 1000 if z > 1000 else z\n",
    "    #z = -1000 if z < -1000 else z\n",
    "    return 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = sigmoida(z)\n",
    "                    #sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #sample_loss +=  - (y * np.log(np.max([sigma, tolerance])) + (1 - y) * np.log(np.max([(1 - sigma), tolerance])))\n",
    "                    sample_loss += -y*np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91a649c0f2c44d7b87c065be95a02e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAAKrCAYAAACZT1JNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01b9695fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.74\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = []\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = sigmoida(z)\n",
    "                    #sigma = 1/(1 + np.exp(-z)) if z >= 0 else 1 - 1/(1 + np.exp(z))\n",
    "                    if sigma > 0.9:\n",
    "                        predicted_tags.append(tag)\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #sample_loss +=  - (y * np.log(np.max([sigma, tolerance])) + (1 - y) * np.log(np.max([(1 - sigma), tolerance])))\n",
    "                    sample_loss += -y*np.log(np.max([tolerance, sigma])) if y == 1 else \\\n",
    "                                   -(1 - y)*np.log(1 - np.min([1 - tolerance, sigma]))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1                \n",
    "                        \n",
    "                self._loss.append(sample_loss)                \n",
    "                if n >= top_n_train:\n",
    "                    sample_acc = len(set.intersection(set(tags), set(predicted_tags))) / len(set.union(set(tags), set(predicted_tags)))                \n",
    "                    self._acc.append(sample_acc) \n",
    "                \n",
    "        return np.average(self._acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b517b2c2c57c4ccc8ba97651a766b8b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
